[["{\"unique_id\":3", "\"candidate_name\":\"Parul_Paul_Techment\"", "\"role\":\"data_engineer\"", "\"description\":\"Techment - Data Engineer Parul Paul   Parul is currently working as a Software Engineer. Having experience in creating automation scripts to efficiently convert data from different formats into the required curated format using Python", " migrating data from diverse sources and seamlessly ingesting them into Azure Databricks", " designing ETL pipelines utilizing various tools", " including Azure Data Factory and Fivetran etc. Driving business transformation through innovative data solutions. Domain Knowledge:  Geographic Information Systems (GIS) Data Financial Domain Health Care Retail Industry Core Skills : Python", " MySQL", "Azure Services like Azure Databricks", " Azure Data Factory", " Azure Active Directory", " Azure SQL", " PySpark", " Snowflake", " Oracle Cloud Infrastructure Overview(Basic).  Programming: Python", " MySQL.  Tools: Jupyter Notebook", " Fivetran", " Postman API", " Visual studio code", " SQL Server Management Studio. DataWarehouse: Snowflake.  Framework: Django (Basic) ETL Tool: Fivetran", "Azure data Factory.  Database: Azure SQL DB", "PostgreSQL", " SQL Server", " MySQL. Repository: Github  Desktop Platform: Window Project executed:  Organization: Techment Technology Client: Enovation Controls", " USA Description: Enovation Controls is an innovative manufacturer of electronic controls and displays for diverse markets. For that created automation script for converting the different types of data ( KML", " KMZ", " GPX", " Geo-PDF) into desired curated format i.e. GPX format that could be readable in Google Earth Pro and QGIS. Languages: Python Environment: Jupyter Notebook DataSources : Different file formats -  KML", " KMZ", " GPX", " Geo-PDF. Methodologies: Agile Role and Responsibilities:                                                                                                                                             Created different automation scripts using Python for curating the existing data files of  different data types KML", " KMZ", " GPX ", " Geo-PDF into the required curated GPX format for states Entire US for type USFS", " ", "Utah for type BLM", " Alaska", " Minnesota for type DNR", " Utah for type State and Local", " Colorado", " Utah for Public Contributor . Validating the conversion process is as expected like Validate trail Name matches file name", "  data structure is correct ", " OHV rated etc. Validated the curated files created by automation script using software tools : QGIS ", " Google Earth Pro.Data Collection of private parks for GIS OHV trail data for the state of USA. Organization: Techment Technology  Client: Growisto Destination: Azure SQL Databases. Description: Growisto is a Digital Growth company that specializes in technology and marketing services. For that", " created the Azure Data Factory (ADF) pipelines for migrating the data from different sources into Azure SQL Databases. Languages: MySQL Environment: Azure Data Factory", " SQL Server Management Studio.  DataSources: ComputerEase", " Quickbooks", " Spectrum", " Sage 100", " Sage 50", " Foundation", " Netsuite", " Sage Intacct", " Quickbase. Methodologies: Agile   Role and Responsibilities:  Migration of different data from different sources into Azure SQL Database using Azure Data Factory. Quickbase Integration: Established  connection for Quickbase in ADF using ODBC Connector and  migrated the data successfully as for few sources there are no connectors available in ADF. Modification and transformation in the existing ADF pipeline. Manual Data Mapping  and validation of the data available in the source and in the database. Organization: Techment Technology Client: Well Doc ", "Bangalore Description:Welldoc is the  Leading digital health platform revolutionizing chronic conditionmanagement to help transform lives. In the Welldoc data analytics platform", " created an end to end pipeline for Data Migration ", " i.e. different data sources and ingesting them into Azure Databricks by creating ETL pipelines and sharing the  live data using Delta Sharing. Languages: SQL", " Python", " Pyspark.  Environment: Azure Databricks ", " Azure data factory", " Azure Data Lake Gen 2 etc.  DataSources: Application Events (Historical Data and Live Streaming Data)", " Cosmos DB for Mongo API", " SendBird", " Google Analytics", " Salesforce Destination: Databricks Methodologies: Agile   Role and Responsibilities:                                                                                                                                               Migrating data from various data sources. For Historical Data : Getting the data from Table Storage in ADLS using Data Factory and ingesting the data into ADLS container and then reading the data from ADLS into databricks and also performing the required ETL in the pipeline. For Live Streaming Data : Getting the data into Azure Event  and then ingesting the data into databricks. Enabled CDC (Change Data Capture) and CDF(Change Data Feed) in the pipeline for capturing the changes in the data. Delta Sharing using its Unity Catalog data governance platform", " sharing data from databricks to databricks", " using rest APIs and python libraries by sharing credential files to different clients. Worked on Fivetran from ingesting data from sources like Salesforce", " Google Analytics Export 4 etc into  Azure Databricks. Organization: Techment TechnologyClient: Vintage Wine Estate Description:Vintage Wine Estates (VWE) is the fastest-growing family of wineries and wines with a leading direct-to-customer platform", " hosting over 30 wineries under its umbrella. The company sells 2.2 million nine-liter equivalent cases annually and has emerged as a top 15 wine producer in the US via organic and acquisitive growth. Environment: Snowflake", " MySql", " AWS S3 bucket", " SQL", " Postgres", " Azure Blob Storage", " Python Role and Responsibilities: Data cleaning. Data ingestion: Load data in MySql", " PostgreSQL", " Azure Blob Storage from Excel Sheet using Python Script Data Migration: Data from different sources to S3 into snowflake using snowpipe. Incremental Loading in Snowflake using Task/ Scheduler. Fivetran connection through Mysql and Postgres ", " loaded data in Snowflake. Incremental Loading in Fivetran Unstructured data(pdf) into structured data", " converted into csv and ingested into snowflake using snowpark. Apache Spark for importing data of different formats and converting it into parquet files to be stored in Azure Databricks Delta Lake. Organization: Techment Technology Client: RCE Organization Description : Milestones Intervention Services (MIS) Platform was an intervention hub created to bring together a network of professionals providing teleservices to clients of all ages and abilities throughout the United States of America. Various provider groups can connect to MIS", " receive a License for using the MIS platform", " create their own network of professionals dedicated tocollaborating and serving the clients using the MIS platform. The MIS platform can be seen as a clinical management platform containing EMR and Tele-practice capabilities. This platform will be used for performing treatments and maintaining all aspects of the therapeutic journey. The MIS platform will follow the Multi-tenant approach where various provider groups will be on-boarded on the MIS platform and they will be enabled to create their own network of professionals who will provide intervention                                                                                                                                                   services to their Clients.  Environment: Java Spring Boot", " Microservices", "Mysql Third Party Library: Agora", " Twilio", " Stripe Role and Responsibilities: Created Rest API\u00e2\u20ac\u2122s for Front end i.e Web Development and Mobile App both Android and Ios. Worked in Twilio for SMS services for different scenarios like sending SMS while session creation", " session update", " session remainder", " session cancellation. Also created API\u00e2\u20ac\u2122s for Email sending in different scenarios. Have done Debugging.\"", "\"file_name\":\"Parul_Paul_Techment.json\"", "\"last_modified\":\"2024-05-22T14:09:47Z\"", "\"object_path\":\"/\"", "\"object_size\":\"8020\"", "\"object_version_id\":\"77fccce7-57b2-4d3f-867f-ae3b97c297b8\"}"], ["{\"unique_id\":4", "\"candidate_name\":\"Ramesh_Kumar\"", "\"role\":\"software_engineer\"", "\"description\":\"Techment - Software Engineer Koti Sai Ramesh Kumar      Ramesh is currently working as a Software engineer. He is a highly motivated and energetic developer with excellent product development and has a fair understanding of generative AI.  Programming Languages: Python", " C#", "Typescript Desktop Platforms: Windows", " Linux Development Tools: Visual Studio Code", " Visual Studio", " Google colab", "Amazon Sagemaker", " Jupyter", " Jetbrains IDE Databases: MySql", " MsSql server", " PineCone", "Faiss(Vector database)", " MongoDb", " PlanetScale Framework/SDK: Django", " Langchain", " Next JS", " .Net Core Other technologies: Generative AI", " Snowflake", " ArcGis pro Design/Architectural Patterns: MVT(Model View Template)", " MVC", " MVVM Projects executed:     Organization: Techment technology Pvt Ltd Description: Using the Langchain framework developed a retrieval augmented generation application on top of Generative AI where our client needed to interact with a Large language model which will have a knowledge base augmented in the form of PDF. We created an interface using Streamlit at frontend where the user is asked to upload their Pdf document. Python script is writtenin such a way that the text is extracted from uploaded Pdf and is embedded to store in Vector Db(Faiss/Pinecone). Further when the user asks a question to the LLM", " the algorithm performs similarity search to find relevant answers and feeds it to LLM to generate a natural language response.I have tested this application using both GPT4 and Llama2. Platforms: Langchain", "PineCone/Faiss", "GPT4/Llama2", "Hugging face", "Streamlit Development Language: Python Role and Responsibilities: Writing Python scripts for the given use case. Testing the most suitable LLM for the above use case. Designing the user interface using Streamlit. Organization: Techment technology Pvt Ltd Description: Using .Net Core MVC developed a movie rental web application in which admin can add", "remove", "update movie details and can monitor payment details of rented movies from users.On the other hand users can view", "rent", "add to cart movies", "while renting the movies the users can make payment for which Razorpay payment gateway is provided to them. Platforms: .Net core Mvc", "Ext js", "Sql Server Development Language: C# Role and Responsibilities: Admin and User panel functionality and designing                                                                                                                                Admin and User authentication and authorization Database architecture design Payment gateway integration Organization: Techment technology Pvt LtdDescription: Using .Net core and Wpf developed a leave management system where organization users can apply for leave", " selecting reasons for leave and the admin( manager", "HR) can decline or approve the leaves applied. Platforms: .Net core", "wpf", "SQL server Development Language: C# Role and Responsibilities: Admin panel functionality and designing Admin Database architecture design Admin and User database authentication and authorization Organization: Techment technology Pvt Ltd Description: Using Typescript TRPC in backend and React Next JS in frontend we have developed a learning management system where there were two separate interfaces for teacher/mentor who can assign tasks to students", "monitor their progress and conduct tests where in students can access the courses provided subscribe courses and attend tests for the same. Platforms: NextJS", "TRPC", "Prisma ORM", "Planet Scale Db Development Language: Typescript Role and Responsibilities: Admin panel functionality and designing Admin Database architecture design Admin and User database authentication and authorization using Clerk hosted service Role based authorization Payment gateway integration using Stripe Organization: Techment technology Pvt Ltd Description: Using .Net Framework for desktop and react for web developed an application whichaccesses the ArcGis server to fetch Spatial data. Customized the application so that a user can utilize the application without having a specialization in Gis. Platforms: .Net", "React", "ArcGis Pro/Online Development Language: .Net", " Javascript Role and Responsibilities: Integration of ArcGis with React and .Net application Studying use of Spatial data Organization: Techment technology Pvt Ltd                                                                                                                                                                 Description: Using Zia skills in Zoho", " SalesIQ developed a chatbot for our company\u00e2\u20ac\u2122s website. The bot was built using a scripting language called Deluge. The bot acts as a virtual assistant on our company\u00e2\u20ac\u2122s website. Platforms: ZiaSkills", "SalesIQ Development Language: Deluge Role and Responsibilities: Writing  scripts for the bot actions Feeding keywords to invoke bots action Writing prompts for the bot\u00e2\u20ac\u2122s response B.Tech (CSE) from Raipur Institute of Technology Engineering and Entrepreneurship (RITEE)", "Raipur(C.G) \"", "\"file_name\":\"Ramesh_Kumar.json\"", "\"last_modified\":\"2024-05-22T14:09:48Z\"", "\"object_path\":\"/\"", "\"object_size\":\"5113\"", "\"object_version_id\":\"128357f1-7412-42a9-a88c-1031585f83d6\"}"], ["{\"unique_id\":2", "\"candidate_name\":\"Krishna_Sahu\"", "\"role\":\"data_engineer\"", "\"description\":\" Techment - Data Engineer Krishna Sahu   Krishna is currently working as a Software Engineer. Having 1.5 years of experience in Data engineering and 1.5 years of experience in Software development. Having experience in creating automation scripts to efficiently convert data from different formats into the required curated format using Python", " migrating data from diverse sources and seamlessly ingesting them into Azure Databricks", " designing ETL pipelines utilizing various tools", " including Azure Data Factory and Fivetran etc.Objective is to pursue a career as a Data Engineer  at Techment where skills in coding will be useful while delivering a service to the organization and to be expertise in both aspects of technology and business in order to contribute efforts for meeting organizational goals.  Domain Knowledge:  Data engineering Data Ingestion and Data migration Health Care  Core Skills: Python", " Azure Databricks", " PySpark", " MySQL", " ETL Tools:  Azure Databricks", " Azure Data Factory ", " Fivetran", " Snowflake", " ", "Azure Event Hub", "  Azure SQL", " Snowflake Databases: MySQL(intermediate)", "  MongoDB(Beginner)", "  Postgresql(Beginner) Cloud Computing Platform Tools: Azure  Databricks with Apache Spark", " Delta lake", " Blob Storage", " AWS S3 Bucket", " Snowflake", " Snowpark Desktop Platform: Windows Development Tools: VS Code", " PyCharm", " Eclipse", "  Jupyter Notebook Version Control Tools: Github", " SVN Defect Tracking Tool: JIRA", " Asana Other Skills: Written and Verbal Communication", " Teamwork Projects executed:    Organization: Techment Technology Client: WellDoc", " Bangalore Description: Welldoc is the  Leading digital health platform revolutionizing chronic condition management to help transform lives. In the Welldoc data analytics platform", " created an end to end pipeline for Data Migration ", " i.e. different data sources and ingesting them into Azure Databricks by creating ETL pipelines and sharing the  live data using Delta Sharing. Languages: SQL", " Python", " Pyspark.  Environment: Azure Databricks ", " Azure data factory", " Azure Data Lake Gen 2", " Azure Event Hub", " Datavant  etc.                                                                                                                                              \\nData Sources : Application Events (Historical Data and Live Streaming Data)", " Cosmos DB for Mongo API", " SendBird", " Google Analytics", " Salesforce Destination : Databricks Methodologies: Agile   Role and Responsibilities:  Data ingestion and Data migration from various data sources: Application Events (Historical Data and Live Streaming Data)", " Cosmos DB for Mongo API", " SendBird", " Google Analytics", " Salesforce. Historical Data ingestion: Getting the data from Table Storage in ADLS using Data Factory and ingesting the data into ADLS container and then reading the data from ADLS into databricks and also performing the  required ETL in the pipeline. Streaming Data ingestion: Getting the data into Azure Event  and then ingesting the data into     databricks. Change Data Capture: Enabled CDC (Change Data Capture) and CDF(Change Data Feed) in the pipeline for capturing the  changes in the data. Data Encryption / transformation: Worked with encryption", " decryption and re-encryption of delta tables of sales force data according to need.  Delta Sharing: It uses its Unity Catalog data governance platform", " sharing data from databricks to    databricks", " using rest APIs and python libraries by sharing credential files to different clients. Fivetran (ETL): Worked on Fivetran from ingesting data from sources like Salesforce", " Google Analytics Export 4 etc into  Azure Databricks. Tokenization and transformation: To  securely share Personally Identifiable Information (PII) and Protected health information (PHI). We created a Payer data Export app where we share PHI data as attributes lists. But as we need to follow HIPAA compliance HIPAA rules are a set of guidelines and regulations established to protect the confidentiality", " integrity", " and availability of electronically protected health information. So we need to apply Tokenization to protect PII and PHI data. So we created a Payer data export extension application where we perform Tokenization on PII data then share data to clients. Client will select the tokens and send them back to us. We matched the sent and received tokens then attached the PHI information to matched tokens and then sent encrypted PHI back to them.   Organization: Techment Technology Client: Snavely Associates", " USA Description : Ovrture is a digital platform built for the top of the gift pyramid. You can create tailored cultivation sites for sharing key information (proposals", " gift agreements", " personal videos", " specific fundraising priorities) with prospects.\\nEnvironment: Eclipse", " Java", " Springboot", "Github", " VS Code Role and Responsibilities:  Execute full software development life cycle (SDLC) Planout of the task for requirement. Write well-designed", " testable code. Produce specifications and determine operational feasibility. Ensure the well written and optimized code quality. Attending Agile meetings.    Organization: Techment Technology Client: Vintage Wine Estates (VWE)                                                                                                                                                      Description : Vintage Wine Estates (VWE) is the fastest-growing family of wineries and wines with a leading direct-to-customer platform", " hosting over 30 wineries under its umbrella. The company sells 2.2 million nine-liter equivalent cases annually and has emerged as a top 15 wine producer in the US via organic and acquisitive growth. Environment:  Snowflake", " MySql", " AWS s3 bucket", " SQL", " Postgres", " Azure Blob Storage", " Python Role and Responsibilities:  Data Ingestion into Postgresql from Excel sheets. Data transfer into S3 bucket through python scripts. Migration of data from s3 Bucket into snowflake via snowpipe. Performed incremental loading via python scripts  with the help of stream/task into snowflake. Established  Fivetran connection between Postgresql and snowflake.  Data Injection from postgresql into Azure Blob Storage. Apache Spark for importing Data from different formats and converting it into parquet files to be stored in Azure Databricks Delta Lake.\"", "\"file_name\":\"Krishna_Sahu.json\"", "\"last_modified\":\"2024-05-22T14:09:47Z\"", "\"object_path\":\"/\"", "\"object_size\":\"6368\"", "\"object_version_id\":\"4c373c06-e7e5-4608-8885-6f45d67c69b9\"}"], ["{\"unique_id\":5", "\"candidate_name\":\"Shubham_Gavel\"", "\"role\":\"quality_assurance\"", "\"description\":\"Shubham Gavel 123 Quality Ave QA City", " QC 12345 shubhamgavel@email.com (123) 456-7890  Professional Summary: A diligent and detail-oriented Quality Assurance professional with 5 years of experience in ensuring product quality", " compliance", " and customer satisfaction. Proficient in various quality assurance methodologies", " including Agile and Six Sigma", " with a proven track record of implementing effective quality assurance processes to improve product reliability and performance.  Education: - Bachelor of Science in Computer Science", " QA University", " 2015 - Certified Quality Assurance Professional (CQAP)  Professional Experience: 1. Quality Assurance Analyst - QualityTech Solutions", " Jan 2017 - Present    - Conducted thorough quality assurance testing of software applications", " identifying and documenting defects and inconsistencies.    - Collaborated with cross-functional teams to develop and implement quality assurance processes and procedures.    - Participated in product design and development meetings to provide input on quality requirements and specifications.    - Generated comprehensive reports detailing testing results", " issues found", " and recommendations for improvement.  2. Quality Control Technician - TechCheck Industries", " Aug 2015 - Dec 2016    - Inspected incoming raw materials and components to ensure compliance with quality standards and specifications.    - Conducted routine quality control checks throughout the manufacturing process", " identifying and addressing any deviations from quality standards.    - Collaborated with production teams to troubleshoot quality issues and implement corrective actions.    - Maintained detailed records of quality control activities and communicated findings to relevant stakeholders.  Skills: - Proficient in quality assurance methodologies such as Agile and Six Sigma - Strong analytical and problem-solving skills - Excellent attention to detail and accuracy - Effective communication and interpersonal skills - Ability to work independently and collaboratively in a team environment - Proficient in JIRA", " Selenium", " and HP Quality Center  \"", "\"file_name\":\"Shubham_Gavel.json\"", "\"last_modified\":\"2024-05-22T14:09:47Z\"", "\"object_path\":\"/\"", "\"object_size\":\"2231\"", "\"object_version_id\":\"ac262fd1-7a83-485a-ad15-cc9bca720dab\"}"], ["{\"unique_id\":1", "\"candidate_name\":\"Kajal_Balchandani\"", "\"role\":\"data_engineer\"", "\"description\":\" Techment - Software engineer Kajal Balchandani     Kajal is presently working as a Software Engineer based in the Indore area", " with experience in both Data Science and backend development. She has excelled in collaborating with Data Science teams", " acquiring valuable experience in application development. She is known for her strong analytical skills", " problem-solving abilities", " and contributions to team building and organization.   Backend Framework: Python 3 + Django Rest API framework Technologies: Azure Active Directory", " Azure functions", " Azure Cosmos DB", " Apache OpenNLP", " SciKit-Learn for Machine Learning", " NLTK", " Google Functions Databases: Postgresql", " MySQL", " Firestore", " Big Query Desktop Platform: Windows Development Tools: VSCode", " PyCharm", " MySQL Workbench", " PgAdmin Projects executed:     Organization: Techment Technology Role: Backend Developer Description: Arist is an enterprise project in the data engineering domain. A SaaS-based Data analytics platform", " which will show business insights of Farragut to county users. It empowers county users with valuable business insights", " particularly catering to Farragut's Local Government Solutions. The platform offers Smart Apps", " Web Dashboards", " and Visualization tools", " following a multi-tenant architecture and modular subscription model. Key stakeholders encompass Farragut Admin", " Customer Admin", " and Customer Staff", " facilitating data-driven decisions. Environment: Python", " Django", " Azure SQL Database", " Azure Data Factory", " Azure Functions", " Azure Active Directory ", " Azure ADB2C", " Azure App Service", " Power BI. Role and Responsibilities: Collaborated with a team of 2 backend developers and worked in parallel with a frontend developer to understand the frontend needs and ensure alignment between front and backend systems.  Designed the user management system", " implementing a role-based access control system to govern platform access. Integrated Azure Active Directory to enhance efficiency in managing user roles and permissions seamlessly. Worked on configuring Azure AD B2C to create a customized user flow that allowed users to access the platform after completing multi-factor authentication (MFA). Created Rest APIs using Django Rest Framework", " covering user management system (login", " creation", " update", " and deletion)", " dashboard display", " and data synchronization functionalities for the platform. Created and maintained the database for the application", " working on both PostgreSQL and Microsoft MySQL environments to ensure optimal functionality and data management.                                                                                                                                          Spearheaded the automation of Power BI for seamless integration of dashboards within the application. Direct interaction with the clients and understand the problem statements of each use case and suggest and deliver effective solutions for the same.   Organization: Techment Technology  Role: Backend Developer Description: Root Food Group Proof of Concept utilizing Google Cloud services. Employed Firestore for streaming user data storage", " executed data filtration via Big Query", " and customized visualizations on Google Looker for clinical and sales teams. This initiative refined operational insights", " offering comprehensive patient status views and detailed information access for both user segments. Environment: Python", " Open Tender", " Klaviyo Environment: Python", " Django", " Google Firestore", " Google BigQuery", " Google Looker. Role and Responsibilities:  Stored all streaming data in Firestore", " a NoSQL database", " serving as the primary data source. Streamlined real-time data transfer from Firestore to BigQuery", " converting NoSQL data into a structured format for enhanced accessibility. Linked BigQuery to Looker Studio", " crafting tailored visualizations for specific use cases. Developed Django APIs for user input", " data preprocessing", " filtering", " and displaying Looker visualizations in the application.  Engaged in direct client interactions to identify and address project-related issues and areas of concern.  \"", "\"file_name\":\"Kajal_Balchandani.json\"", "\"last_modified\":\"2024-05-22T14:09:47Z\"", "\"object_path\":\"/\"", "\"object_size\":\"4204\"", "\"object_version_id\":\"946c9c32-47fa-419e-a09d-ea04e73afae6\"}"]]